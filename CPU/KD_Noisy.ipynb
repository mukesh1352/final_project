{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86860bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bde91e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Noise\n",
    "def add_gaussian_noise(img, mean=0, std=0.3):\n",
    "    noise = torch.randn(img.size()) * std + mean\n",
    "    noisy_img = img + noise\n",
    "    return torch.clamp(noisy_img, 0., 1.)\n",
    "\n",
    "# Occlusion Noise (random black box)\n",
    "def add_occlusion(img, box_size=7):\n",
    "    img = img.clone()\n",
    "    _, h, w = img.shape\n",
    "    x = random.randint(0, w - box_size)\n",
    "    y = random.randint(0, h - box_size)\n",
    "    img[:, y:y+box_size, x:x+box_size] = 0.0\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3718de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyMNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, noise_type='gaussian'):\n",
    "        self.data = base_dataset\n",
    "        self.noise_type = noise_type\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.data[index]\n",
    "        if self.noise_type == 'gaussian':\n",
    "            img = add_gaussian_noise(img)\n",
    "        elif self.noise_type == 'occlusion':\n",
    "            img = add_occlusion(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Load original MNIST\n",
    "transform = transforms.ToTensor()\n",
    "train_clean = datasets.MNIST(root='./data1', train=True, download=True, transform=transform)\n",
    "test_clean = datasets.MNIST(root='./data1', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create noisy versions\n",
    "train_noisy = NoisyMNISTDataset(train_clean, noise_type='gaussian')  # or 'occlusion'\n",
    "test_noisy  = NoisyMNISTDataset(test_clean, noise_type='gaussian')\n",
    "\n",
    "train_loader = DataLoader(train_noisy, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_noisy, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f943acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class StudentCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 * 7 * 7, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac54c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss(student_logits, teacher_logits, labels, T=3, alpha=0.7):\n",
    "    soft_loss = nn.KLDivLoss(reduction='batchmean')(\n",
    "        F.log_softmax(student_logits / T, dim=1),\n",
    "        F.softmax(teacher_logits / T, dim=1)\n",
    "    ) * (T * T)\n",
    "    hard_loss = F.cross_entropy(student_logits, labels)\n",
    "    return alpha * soft_loss + (1 - alpha) * hard_loss\n",
    "\n",
    "def train_kd(student, teacher, optimizer, train_loader, test_loader, epochs=5):\n",
    "    student = student.to(device)\n",
    "    teacher = teacher.to(device)\n",
    "    teacher.eval()\n",
    "\n",
    "    history = {'epoch':[], 'train_loss':[], 'train_acc':[], 'test_loss':[], 'test_acc':[], 'time':[]}\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        student.train()\n",
    "        running_loss, correct, total = 0, 0, 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                t_logits = teacher(xb)\n",
    "            s_logits = student(xb)\n",
    "            loss = distillation_loss(s_logits, t_logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            _, preds = torch.max(s_logits, 1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += xb.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        test_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        student.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                out = student(xb)\n",
    "                loss = F.cross_entropy(out, yb)\n",
    "                test_loss += loss.item() * xb.size(0)\n",
    "                _, preds = torch.max(out, 1)\n",
    "                correct += (preds == yb).sum().item()\n",
    "                total += xb.size(0)\n",
    "        test_loss /= total\n",
    "        test_acc = correct / total\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        history['epoch'].append(ep)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['time'].append(epoch_time)\n",
    "\n",
    "        print(f\"Epoch {ep} - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    return student, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4deac99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "class TeacherCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "810b6115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, train_loader, test_loader, epochs=5):\n",
    "    model = model.to(device)\n",
    "    history = {'epoch': [], 'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            _, preds = torch.max(out, 1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += xb.size(0)\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                test_loss += loss.item() * xb.size(0)\n",
    "                _, preds = torch.max(out, 1)\n",
    "                correct += (preds == yb).sum().item()\n",
    "                total += xb.size(0)\n",
    "        test_loss /= total\n",
    "        test_acc = correct / total\n",
    "\n",
    "        # Save stats\n",
    "        history['epoch'].append(ep)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "\n",
    "        print(f\"Epoch {ep}: Train Acc={train_acc:.4f}, Test Acc={test_acc:.4f}\")\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aab09394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29cec037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TeacherCNN on clean MNIST...\n",
      "Epoch 1: Train Acc=0.9447, Test Acc=0.9817\n",
      "Epoch 2: Train Acc=0.9845, Test Acc=0.9868\n",
      "Epoch 3: Train Acc=0.9896, Test Acc=0.9890\n",
      "Epoch 4: Train Acc=0.9915, Test Acc=0.9878\n",
      "Epoch 5: Train Acc=0.9941, Test Acc=0.9913\n",
      "âœ… Teacher model saved as 'teacher_pretrained.pth'\n"
     ]
    }
   ],
   "source": [
    "teacher = TeacherCNN().to(device)\n",
    "optimizer = optim.Adam(teacher.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the teacher\n",
    "print(\"Training TeacherCNN on clean MNIST...\")\n",
    "teacher, teacher_history = train_model(teacher, optimizer, criterion, train_loader, test_loader, epochs=5)\n",
    "\n",
    "# Save the model\n",
    "torch.save(teacher.state_dict(), \"teacher_pretrained.pth\")\n",
    "print(\"âœ… Teacher model saved as 'teacher_pretrained.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f464ed58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TeacherCNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher = TeacherCNN().to(device)\n",
    "teacher.load_state_dict(torch.load(\"teacher_pretrained.pth\"))\n",
    "teacher.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04917520",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "687d6626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss(student_logits, teacher_logits, targets, temperature=4.0, alpha=0.7):\n",
    "    kd_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(student_logits / temperature, dim=1),\n",
    "                                                  F.softmax(teacher_logits / temperature, dim=1)) * (temperature ** 2)\n",
    "    ce_loss = F.cross_entropy(student_logits, targets)\n",
    "    return alpha * kd_loss + (1 - alpha) * ce_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91d5c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kd(student, teacher, train_loader, test_loader, epochs=5):\n",
    "    optimizer = optim.Adam(student.parameters(), lr=1e-3)\n",
    "    student = student.to(device)\n",
    "    teacher = teacher.to(device)\n",
    "    teacher.eval()\n",
    "\n",
    "    hist = {'epoch': [], 'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [], 'time': []}\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        student.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        import time\n",
    "        start = time.time()\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            teacher_out = teacher(xb)\n",
    "            student_out = student(xb)\n",
    "\n",
    "            loss = distillation_loss(student_out, teacher_out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "            preds = torch.argmax(student_out, dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += xb.size(0)\n",
    "\n",
    "        train_loss /= total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # --- Evaluate on test set ---\n",
    "        student.eval()\n",
    "        test_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                out = student(xb)\n",
    "                loss = F.cross_entropy(out, yb)\n",
    "                test_loss += loss.item() * xb.size(0)\n",
    "                preds = torch.argmax(out, dim=1)\n",
    "                correct += (preds == yb).sum().item()\n",
    "                total += xb.size(0)\n",
    "\n",
    "        test_loss /= total\n",
    "        test_acc = correct / total\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        # Save stats\n",
    "        hist['epoch'].append(ep)\n",
    "        hist['train_loss'].append(train_loss)\n",
    "        hist['train_acc'].append(train_acc)\n",
    "        hist['test_loss'].append(test_loss)\n",
    "        hist['test_acc'].append(test_acc)\n",
    "        hist['time'].append(elapsed)\n",
    "\n",
    "        print(f\"Epoch {ep}: train_acc={train_acc:.4f}, test_acc={test_acc:.4f}, time={elapsed:.2f}s\")\n",
    "\n",
    "    return student, hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "740718f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“š Training student via knowledge distillation on CLEAN MNIST...\n",
      "Epoch 1: train_acc=0.9279, test_acc=0.9827, time=29.88s\n",
      "Epoch 2: train_acc=0.9856, test_acc=0.9879, time=29.51s\n",
      "Epoch 3: train_acc=0.9896, test_acc=0.9890, time=29.89s\n",
      "Epoch 4: train_acc=0.9914, test_acc=0.9899, time=30.48s\n",
      "Epoch 5: train_acc=0.9926, test_acc=0.9895, time=30.25s\n"
     ]
    }
   ],
   "source": [
    "teacher = TeacherCNN().to(device)\n",
    "teacher.load_state_dict(torch.load(\"teacher_pretrained.pth\"))\n",
    "teacher.eval()\n",
    "\n",
    "student = StudentCNN().to(device)\n",
    "\n",
    "print(\"\\nðŸ“š Training student via knowledge distillation on CLEAN MNIST...\")\n",
    "student, hist_kd_clean = train_kd(student, teacher, train_loader, test_loader, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57ee16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add Gaussian noise to images\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=0.2):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.randn(tensor.size()) * self.std + self.mean\n",
    "        noisy_tensor = tensor + noise\n",
    "        return torch.clamp(noisy_tensor, 0., 1.)\n",
    "\n",
    "# === Transforms for training ===\n",
    "transform_noisy = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0., 0.3),         # Adjust std as needed\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "transform_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# === Datasets ===\n",
    "train_noisy = datasets.MNIST(root='./data', train=True, download=True, transform=transform_noisy)\n",
    "test_clean  = datasets.MNIST(root='./data', train=False, download=True, transform=transform_clean)\n",
    "\n",
    "train_loader_noisy = DataLoader(train_noisy, batch_size=128, shuffle=True)\n",
    "test_loader_clean  = DataLoader(test_clean, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0319a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = TeacherCNN().to(device)\n",
    "teacher.load_state_dict(torch.load(\"teacher_pretrained.pth\"))\n",
    "teacher.eval()\n",
    "\n",
    "student_noisy = StudentCNN().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "589b3137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“š Training student via knowledge distillation on NOISY MNIST...\n",
      "Epoch 1: train_acc=0.8356, test_acc=0.9711, time=33.71s\n",
      "Epoch 2: train_acc=0.9418, test_acc=0.9803, time=30.63s\n",
      "Epoch 3: train_acc=0.9449, test_acc=0.9849, time=30.27s\n",
      "Epoch 4: train_acc=0.9465, test_acc=0.9850, time=31.19s\n",
      "Epoch 5: train_acc=0.9473, test_acc=0.9865, time=32.94s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ“š Training student via knowledge distillation on NOISY MNIST...\")\n",
    "student_noisy, hist_kd_noisy = train_kd(student_noisy, teacher, train_loader_noisy, test_loader_clean, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c5f1dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Metric  Clean KD  Noisy KD\n",
      "0          Final Train Loss    0.0886    0.1920\n",
      "1           Final Test Loss    0.0306    0.0416\n",
      "2  Final Train Accuracy (%)   99.2600   94.7300\n",
      "3   Final Test Accuracy (%)   98.9500   98.6500\n",
      "4   Total Training Time (s)  150.0100  158.7500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Simulated data structure (replace with your actual history dicts)\n",
    "# hist_kd_clean = {'train_loss': [...], 'test_loss': [...], 'train_acc': [...], 'test_acc': [...], 'time': [...]}\n",
    "# hist_kd_noisy = {'train_loss': [...], 'test_loss': [...], 'train_acc': [...], 'test_acc': [...], 'time': [...]}\n",
    "\n",
    "# Final values\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Final Train Loss',\n",
    "        'Final Test Loss',\n",
    "        'Final Train Accuracy (%)',\n",
    "        'Final Test Accuracy (%)',\n",
    "        'Total Training Time (s)'\n",
    "    ],\n",
    "    'Clean KD': [\n",
    "        round(hist_kd_clean['train_loss'][-1], 4),\n",
    "        round(hist_kd_clean['test_loss'][-1], 4),\n",
    "        round(hist_kd_clean['train_acc'][-1] * 100, 2),\n",
    "        round(hist_kd_clean['test_acc'][-1] * 100, 2),\n",
    "        round(sum(hist_kd_clean['time']), 2)\n",
    "    ],\n",
    "    'Noisy KD': [\n",
    "        round(hist_kd_noisy['train_loss'][-1], 4),\n",
    "        round(hist_kd_noisy['test_loss'][-1], 4),\n",
    "        round(hist_kd_noisy['train_acc'][-1] * 100, 2),\n",
    "        round(hist_kd_noisy['test_acc'][-1] * 100, 2),\n",
    "        round(sum(hist_kd_noisy['time']), 2)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the table\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display in notebook or save as CSV\n",
    "print(df_comparison)\n",
    "\n",
    "# Optional: Save to file\n",
    "df_comparison.to_csv(\"kd_clean_vs_noisy_comparison.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "731c2bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/6rfs2rgd6hgcnbhfs77mzc9m0000gn/T/ipykernel_2941/3005949242.py:21: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/var/folders/21/6rfs2rgd6hgcnbhfs77mzc9m0000gn/T/ipykernel_2941/3005949242.py:36: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/var/folders/21/6rfs2rgd6hgcnbhfs77mzc9m0000gn/T/ipykernel_2941/3005949242.py:49: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# Prevent kernel crash in VS Code if needed\n",
    "matplotlib.use('Agg')  # <- remove this line if you want interactive display\n",
    "\n",
    "epochs = hist_kd_clean['epoch']\n",
    "\n",
    "# === Accuracy ===\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, hist_kd_clean['train_acc'], 'b-o', label=\"Clean KD - Train Acc\")\n",
    "plt.plot(epochs, hist_kd_clean['test_acc'],  'b--', label=\"Clean KD - Test Acc\")\n",
    "plt.plot(epochs, hist_kd_noisy['train_acc'], 'r-o', label=\"Noisy KD - Train Acc\")\n",
    "plt.plot(epochs, hist_kd_noisy['test_acc'],  'r--', label=\"Noisy KD - Test Acc\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Train vs Test Accuracy: Clean vs Noisy KD\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"kd_accuracy_comparison.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# === Loss ===\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, hist_kd_clean['train_loss'], 'b-o', label=\"Clean KD - Train Loss\")\n",
    "plt.plot(epochs, hist_kd_clean['test_loss'],  'b--', label=\"Clean KD - Test Loss\")\n",
    "plt.plot(epochs, hist_kd_noisy['train_loss'], 'r-o', label=\"Noisy KD - Train Loss\")\n",
    "plt.plot(epochs, hist_kd_noisy['test_loss'],  'r--', label=\"Noisy KD - Test Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train vs Test Loss: Clean vs Noisy KD\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"kd_loss_comparison.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# === Time ===\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, hist_kd_clean['time'], 'b-o', label=\"Clean KD Time\")\n",
    "plt.plot(epochs, hist_kd_noisy['time'], 'r-o', label=\"Noisy KD Time\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"Training Time per Epoch: Clean vs Noisy KD\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"kd_time_comparison.png\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68453bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
