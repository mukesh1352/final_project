{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abb6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision matplotlib scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1af5ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8477f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64*7*7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64*7*7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3adb71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "train_data = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader_clean = DataLoader(test_data, batch_size=1000)\n",
    "\n",
    "def add_noise(tensor, std=0.5):\n",
    "    return torch.clip(tensor + torch.randn_like(tensor) * std, 0., 1.)\n",
    "\n",
    "noisy_test_images = add_noise(test_data.data.unsqueeze(1).float() / 255.)\n",
    "test_loader_noisy = DataLoader(TensorDataset(noisy_test_images, test_data.targets), batch_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caaeacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(model, loader, optimizer, criterion, epochs=3):\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            _, pred = torch.max(out, 1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    end = time.time()\n",
    "    return 100 * correct / total, end - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efe46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_weight_sharing(model, bits=8):\n",
    "    unique_weights = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name and len(param.data.size()) > 1:\n",
    "            flat = param.data.cpu().numpy().flatten()\n",
    "            unique_weights.extend(flat)\n",
    "\n",
    "    unique_weights = np.array(unique_weights).reshape(-1, 1)\n",
    "    kmeans = KMeans(n_clusters=2**bits, n_init=1).fit(unique_weights)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name and len(param.data.size()) > 1:\n",
    "                shape = param.data.shape\n",
    "                flat = param.data.cpu().numpy().flatten().reshape(-1, 1)\n",
    "                clusters = kmeans.predict(flat)\n",
    "                new_data = centroids[clusters].reshape(shape)\n",
    "                param.data.copy_(torch.tensor(new_data, dtype=param.dtype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa58fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train(model, train_loader, optimizer, criterion, epochs=3)\n",
    "\n",
    "# Apply weight sharing\n",
    "apply_weight_sharing(model, bits=6)\n",
    "\n",
    "# Evaluate\n",
    "acc_clean, t_clean = evaluate(model, test_loader_clean)\n",
    "acc_noisy, t_noisy = evaluate(model, test_loader_noisy)\n",
    "\n",
    "torch.save(model.state_dict(), \"ws_hash_model.pth\")\n",
    "size_mb = os.path.getsize(\"ws_hash_model.pth\") / (1024 ** 2)\n",
    "\n",
    "print(f\"âœ… Accuracy Clean: {acc_clean:.2f}% | Time: {t_clean:.2f}s\")\n",
    "print(f\"âœ… Accuracy Noisy: {acc_noisy:.2f}% | Time: {t_noisy:.2f}s\")\n",
    "print(f\"ðŸ“¦ Model Size: {size_mb:.2f} MB\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
