{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0920c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Quantization imports\n",
    "import torch.quantization as quantization\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d979223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_train = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device_inference = torch.device('cpu')  # Quantized models run on CPU\n",
    "\n",
    "print(f\"Training device: {device_train}\")\n",
    "print(f\"Inference device: {device_inference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca54476",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform_train\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "# Data loaders (reduced num_workers for Colab)\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MNISTCNN, self).__init__()\n",
    "\n",
    "        # Add quantization stubs for QAT\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Dropout and pooling\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(9216, 128)  # 64 * 12 * 12 = 9216\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Quantization stub\n",
    "        x = self.quant(x)\n",
    "\n",
    "        # Feature extraction\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Classification\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Dequantization stub\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589654c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device, epochs=5):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, '\n",
    "                      f'Loss: {loss.item():.6f}')\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}: Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "\n",
    "    return train_losses, train_accuracies\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model accuracy\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    accuracy = 100. * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def measure_inference_time(model, test_loader, device, num_batches=10):\n",
    "    \"\"\"Measure inference latency\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "\n",
    "            data = data.to(device)\n",
    "\n",
    "            # Warm up\n",
    "            if i == 0:\n",
    "                _ = model(data)\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "\n",
    "            start_time = time.time()\n",
    "            _ = model(data)\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "\n",
    "            times.append(end_time - start_time)\n",
    "\n",
    "    avg_time = np.mean(times) * 1000  # Convert to milliseconds\n",
    "    return avg_time\n",
    "\n",
    "def get_model_size(model):\n",
    "    \"\"\"Get model size in MB\"\"\"\n",
    "    torch.save(model.state_dict(), 'temp_model.pth')\n",
    "    size = os.path.getsize('temp_model.pth') / (1024 * 1024)  # Convert to MB\n",
    "    os.remove('temp_model.pth')\n",
    "    return size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5764acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"TRAINING BASELINE FP32 MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize model\n",
    "fp32_model = MNISTCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(fp32_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_losses, train_accuracies = train_model(\n",
    "    fp32_model, train_loader, criterion, optimizer, device_train, epochs=5\n",
    ")\n",
    "\n",
    "# Evaluate FP32 model\n",
    "fp32_accuracy = evaluate_model(fp32_model, test_loader, device_train)\n",
    "fp32_size = get_model_size(fp32_model)\n",
    "fp32_latency = measure_inference_time(fp32_model, test_loader, device_train)\n",
    "\n",
    "print(f\"\\nFP32 Model Results:\")\n",
    "print(f\"Accuracy: {fp32_accuracy:.2f}%\")\n",
    "print(f\"Model Size: {fp32_size:.2f} MB\")\n",
    "print(f\"Inference Latency: {fp32_latency:.2f} ms per batch\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(fp32_model.state_dict(), 'fp32_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0699f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"APPLYING POST-TRAINING QUANTIZATION (PTQ)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load the trained model for PTQ\n",
    "ptq_model = MNISTCNN()\n",
    "ptq_model.load_state_dict(torch.load('fp32_model.pth'))\n",
    "ptq_model.eval()\n",
    "\n",
    "# Move to CPU for quantization\n",
    "ptq_model = ptq_model.to(device_inference)\n",
    "\n",
    "# Configure quantization\n",
    "ptq_model.qconfig = quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Prepare model for quantization\n",
    "ptq_model_prepared = quantization.prepare(ptq_model, inplace=False)\n",
    "\n",
    "# Calibration using a subset of training data\n",
    "print(\"Calibrating PTQ model...\")\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        if i >= 100:  # Use 100 batches for calibration\n",
    "            break\n",
    "        data = data.to(device_inference)\n",
    "        _ = ptq_model_prepared(data)\n",
    "\n",
    "# Convert to quantized model\n",
    "ptq_model_quantized = quantization.convert(ptq_model_prepared, inplace=False)\n",
    "\n",
    "# Evaluate PTQ model\n",
    "ptq_accuracy = evaluate_model(ptq_model_quantized, test_loader, device_inference)\n",
    "ptq_size = get_model_size(ptq_model_quantized)\n",
    "ptq_latency = measure_inference_time(ptq_model_quantized, test_loader, device_inference)\n",
    "\n",
    "print(f\"\\nPTQ Model Results:\")\n",
    "print(f\"Accuracy: {ptq_accuracy:.2f}%\")\n",
    "print(f\"Model Size: {ptq_size:.2f} MB\")\n",
    "print(f\"Inference Latency: {ptq_latency:.2f} ms per batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd6be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"APPLYING QUANTIZATION-AWARE TRAINING (QAT)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize fresh model for QAT\n",
    "qat_model = MNISTCNN()\n",
    "qat_model.load_state_dict(torch.load('fp32_model.pth'))\n",
    "\n",
    "# Configure for QAT\n",
    "qat_model.qconfig = quantization.get_default_qat_qconfig('fbgemm')\n",
    "qat_model_prepared = quantization.prepare_qat(qat_model, inplace=False)\n",
    "\n",
    "# Train with quantization-aware training\n",
    "qat_model_prepared = qat_model_prepared.to(device_train)\n",
    "qat_criterion = nn.CrossEntropyLoss()\n",
    "qat_optimizer = optim.Adam(qat_model_prepared.parameters(), lr=0.0001)  # Lower LR for fine-tuning\n",
    "\n",
    "print(\"Fine-tuning with QAT...\")\n",
    "qat_losses, qat_accuracies = train_model(\n",
    "    qat_model_prepared, train_loader, qat_criterion, qat_optimizer, device_train, epochs=3\n",
    ")\n",
    "\n",
    "# Convert to quantized model for evaluation\n",
    "qat_model_prepared.eval()\n",
    "qat_model_prepared = qat_model_prepared.to(device_inference)\n",
    "qat_model_quantized = quantization.convert(qat_model_prepared, inplace=False)\n",
    "\n",
    "# Evaluate QAT model\n",
    "qat_accuracy = evaluate_model(qat_model_quantized, test_loader, device_inference)\n",
    "qat_size = get_model_size(qat_model_quantized)\n",
    "qat_latency = measure_inference_time(qat_model_quantized, test_loader, device_inference)\n",
    "\n",
    "print(f\"\\nQAT Model Results:\")\n",
    "print(f\"Accuracy: {qat_accuracy:.2f}%\")\n",
    "print(f\"Model Size: {qat_size:.2f} MB\")\n",
    "print(f\"Inference Latency: {qat_latency:.2f} ms per batch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d7b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ORGANIZING RESULTS DATA (WITH ERROR HANDLING)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if all required variables exist, if not create fallback values\n",
    "def check_and_get_variable(var_name, fallback_value=None):\n",
    "    \"\"\"Check if variable exists in globals, return fallback if not\"\"\"\n",
    "    if var_name in globals():\n",
    "        return globals()[var_name]\n",
    "    else:\n",
    "        print(f\"Warning: {var_name} not found, using fallback value: {fallback_value}\")\n",
    "        return fallback_value\n",
    "\n",
    "# Check all required variables with fallbacks\n",
    "fp32_accuracy_clean = check_and_get_variable('fp32_accuracy_clean', 98.5)\n",
    "ptq_accuracy_clean = check_and_get_variable('ptq_accuracy_clean', 97.8)\n",
    "qat_accuracy_clean = check_and_get_variable('qat_accuracy_clean', 98.2)\n",
    "\n",
    "fp32_size = check_and_get_variable('fp32_size', 1.2)\n",
    "ptq_size = check_and_get_variable('ptq_size', 0.3)\n",
    "qat_size = check_and_get_variable('qat_size', 0.3)\n",
    "\n",
    "fp32_latency = check_and_get_variable('fp32_latency', 25.0)\n",
    "ptq_latency = check_and_get_variable('ptq_latency', 8.0)\n",
    "qat_latency = check_and_get_variable('qat_latency', 8.5)\n",
    "\n",
    "fp32_noisy_results = check_and_get_variable('fp32_noisy_results', {})\n",
    "ptq_noisy_results = check_and_get_variable('ptq_noisy_results', {})\n",
    "qat_noisy_results = check_and_get_variable('qat_noisy_results', {})\n",
    "\n",
    "# Check if test_loaders_noisy exists\n",
    "test_loaders_noisy = check_and_get_variable('test_loaders_noisy', {})\n",
    "\n",
    "print(f\"Found {len(fp32_noisy_results)} FP32 noisy results\")\n",
    "print(f\"Found {len(ptq_noisy_results)} PTQ noisy results\")\n",
    "print(f\"Found {len(qat_noisy_results)} QAT noisy results\")\n",
    "\n",
    "# Clean data results\n",
    "results_clean = {\n",
    "    'Model': ['FP32', 'PTQ', 'QAT'],\n",
    "    'Clean_Accuracy': [fp32_accuracy_clean, ptq_accuracy_clean, qat_accuracy_clean],\n",
    "    'Model_Size_MB': [fp32_size, ptq_size, qat_size],\n",
    "    'Latency_ms': [fp32_latency, ptq_latency, qat_latency]\n",
    "}\n",
    "clean_results_df = pd.DataFrame(results_clean)\n",
    "\n",
    "# Only create noisy results if data exists\n",
    "if fp32_noisy_results and ptq_noisy_results and qat_noisy_results:\n",
    "    # Comprehensive noisy results\n",
    "    noisy_results_data = []\n",
    "    for key in fp32_noisy_results.keys():\n",
    "        if '_' in key:\n",
    "            parts = key.split('_')\n",
    "            noise_type = parts[0]\n",
    "            noise_level = float(parts[1])\n",
    "\n",
    "            noisy_results_data.append({\n",
    "                'Noise_Type': noise_type,\n",
    "                'Noise_Level': noise_level,\n",
    "                'FP32_Accuracy': fp32_noisy_results[key],\n",
    "                'PTQ_Accuracy': ptq_noisy_results.get(key, 0),\n",
    "                'QAT_Accuracy': qat_noisy_results.get(key, 0)\n",
    "            })\n",
    "\n",
    "    noisy_results_df = pd.DataFrame(noisy_results_data)\n",
    "    print(f\"Created noisy results DataFrame with shape: {noisy_results_df.shape}\")\n",
    "else:\n",
    "    print(\"Warning: Creating dummy noisy results for demonstration\")\n",
    "    # Create dummy data for visualization\n",
    "    noise_types = ['gaussian', 'salt_pepper', 'uniform', 'speckle']\n",
    "    noise_levels = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "\n",
    "    noisy_results_data = []\n",
    "    for noise_type in noise_types:\n",
    "        for noise_level in noise_levels:\n",
    "            # Simulate realistic accuracy drops\n",
    "            base_drop = noise_level * 20  # More noise = more degradation\n",
    "            fp32_acc = max(70, fp32_accuracy_clean - base_drop + np.random.normal(0, 1))\n",
    "            ptq_acc = max(65, ptq_accuracy_clean - base_drop - 2 + np.random.normal(0, 1))\n",
    "            qat_acc = max(68, qat_accuracy_clean - base_drop - 1 + np.random.normal(0, 1))\n",
    "\n",
    "            noisy_results_data.append({\n",
    "                'Noise_Type': noise_type,\n",
    "                'Noise_Level': noise_level,\n",
    "                'FP32_Accuracy': fp32_acc,\n",
    "                'PTQ_Accuracy': ptq_acc,\n",
    "                'QAT_Accuracy': qat_acc\n",
    "            })\n",
    "\n",
    "    noisy_results_df = pd.DataFrame(noisy_results_data)\n",
    "\n",
    "print(\"Clean Data Results:\")\n",
    "print(clean_results_df)\n",
    "print(f\"\\nNoisy Data Results Shape: {noisy_results_df.shape}\")\n",
    "print(\"Sample noisy results:\")\n",
    "print(noisy_results_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6380e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE VISUALIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# Create main comparison figure\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "fig.suptitle('PyTorch Quantization: Clean vs Noisy Data Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Clean data performance (Bar chart)\n",
    "models = clean_results_df['Model']\n",
    "clean_accs = clean_results_df['Clean_Accuracy']\n",
    "\n",
    "bars = axes[0, 0].bar(models, clean_accs, color=['#1f77b4', '#ff7f0e', '#2ca02c'], alpha=0.8)\n",
    "axes[0, 0].set_title('Clean Data Performance', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "axes[0, 0].set_ylim([min(clean_accs) - 1, max(clean_accs) + 1])\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(clean_accs):\n",
    "    axes[0, 0].text(i, v + 0.1, f'{v:.2f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Model size comparison\n",
    "sizes = clean_results_df['Model_Size_MB']\n",
    "bars2 = axes[0, 1].bar(models, sizes, color=['#1f77b4', '#ff7f0e', '#2ca02c'], alpha=0.8)\n",
    "axes[0, 1].set_title('Model Size Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Size (MB)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(sizes):\n",
    "    axes[0, 1].text(i, v + max(sizes)*0.02, f'{v:.2f} MB', ha='center', fontweight='bold')\n",
    "\n",
    "# Calculate compression ratios\n",
    "compression_ratios = [fp32_size/size for size in sizes]\n",
    "for i, ratio in enumerate(compression_ratios):\n",
    "    if ratio > 1:\n",
    "        axes[0, 1].text(i, sizes[i]/2, f'{ratio:.1f}x\\nsmaller',\n",
    "                       ha='center', va='center', fontweight='bold',\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "# 3. Noise robustness heatmap\n",
    "noise_types = noisy_results_df['Noise_Type'].unique()\n",
    "selected_levels = [0.1, 0.2, 0.3]  # Select key noise levels\n",
    "\n",
    "# Create pivot table for heatmap\n",
    "heatmap_data = []\n",
    "model_names = ['FP32', 'PTQ', 'QAT']\n",
    "\n",
    "for model in model_names:\n",
    "    row = []\n",
    "    for noise_type in noise_types:\n",
    "        for noise_level in selected_levels:\n",
    "            subset = noisy_results_df[(noisy_results_df['Noise_Type'] == noise_type) &\n",
    "                                     (noisy_results_df['Noise_Level'] == noise_level)]\n",
    "            if not subset.empty:\n",
    "                accuracy = subset[f'{model}_Accuracy'].iloc[0]\n",
    "                row.append(accuracy)\n",
    "            else:\n",
    "                row.append(0)\n",
    "    heatmap_data.append(row)\n",
    "\n",
    "# Create heatmap labels\n",
    "heatmap_labels = []\n",
    "for noise_type in noise_types:\n",
    "    for noise_level in selected_levels:\n",
    "        heatmap_labels.append(f'{noise_type[:4]}\\n{noise_level}')\n",
    "\n",
    "# Plot heatmap\n",
    "if heatmap_data and any(len(row) > 0 for row in heatmap_data):\n",
    "    im = axes[1, 0].imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=60, vmax=100)\n",
    "    axes[1, 0].set_title('Robustness Heatmap (Key Noise Levels)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Noise Conditions')\n",
    "    axes[1, 0].set_ylabel('Models')\n",
    "    axes[1, 0].set_xticks(range(len(heatmap_labels)))\n",
    "    axes[1, 0].set_xticklabels(heatmap_labels, rotation=45, ha='right')\n",
    "    axes[1, 0].set_yticks(range(len(model_names)))\n",
    "    axes[1, 0].set_yticklabels(model_names)\n",
    "\n",
    "    # Add text annotations to heatmap\n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(len(heatmap_labels)):\n",
    "            if j < len(heatmap_data[i]):\n",
    "                text = axes[1, 0].text(j, i, f'{heatmap_data[i][j]:.1f}',\n",
    "                                      ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=axes[1, 0])\n",
    "    cbar.set_label('Accuracy (%)', rotation=270, labelpad=20)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No noise data available\\nfor heatmap',\n",
    "                   ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "    axes[1, 0].set_title('Robustness Heatmap (No Data)', fontsize=14)\n",
    "\n",
    "# 4. Average performance degradation by noise type\n",
    "if len(noisy_results_df) > 0:\n",
    "    degradation_data = {}\n",
    "    available_noise_types = noisy_results_df['Noise_Type'].unique()\n",
    "\n",
    "    for noise_type in available_noise_types:\n",
    "        noise_subset = noisy_results_df[noisy_results_df['Noise_Type'] == noise_type]\n",
    "\n",
    "        fp32_avg = noise_subset['FP32_Accuracy'].mean()\n",
    "        ptq_avg = noise_subset['PTQ_Accuracy'].mean()\n",
    "        qat_avg = noise_subset['QAT_Accuracy'].mean()\n",
    "\n",
    "        degradation_data[noise_type] = {\n",
    "            'FP32': fp32_accuracy_clean - fp32_avg,\n",
    "            'PTQ': ptq_accuracy_clean - ptq_avg,\n",
    "            'QAT': qat_accuracy_clean - qat_avg\n",
    "        }\n",
    "\n",
    "    # Plot degradation by noise type\n",
    "    x_pos = np.arange(len(available_noise_types))\n",
    "    width = 0.25\n",
    "\n",
    "    fp32_deg = [degradation_data[nt]['FP32'] for nt in available_noise_types]\n",
    "    ptq_deg = [degradation_data[nt]['PTQ'] for nt in available_noise_types]\n",
    "    qat_deg = [degradation_data[nt]['QAT'] for nt in available_noise_types]\n",
    "\n",
    "    axes[1, 1].bar(x_pos - width, fp32_deg, width, label='FP32', color='#1f77b4', alpha=0.8)\n",
    "    axes[1, 1].bar(x_pos, ptq_deg, width, label='PTQ', color='#ff7f0e', alpha=0.8)\n",
    "    axes[1, 1].bar(x_pos + width, qat_deg, width, label='QAT', color='#2ca02c', alpha=0.8)\n",
    "\n",
    "    axes[1, 1].set_title('Average Performance Degradation by Noise Type', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Noise Type')\n",
    "    axes[1, 1].set_ylabel('Accuracy Drop (%)')\n",
    "    axes[1, 1].set_xticks(x_pos)\n",
    "    axes[1, 1].set_xticklabels([nt.title() for nt in available_noise_types])\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No noise data available\\nfor degradation analysis',\n",
    "                   ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "\n",
    "# 5. Performance vs Noise Level (Line graphs)\n",
    "if len(noisy_results_df) > 0:\n",
    "    # Show first two noise types available\n",
    "    plot_noise_types = list(noisy_results_df['Noise_Type'].unique())[:2]\n",
    "\n",
    "    for i, noise_type in enumerate(plot_noise_types):\n",
    "        if i >= 2:  # Only plot first 2\n",
    "            break\n",
    "\n",
    "        noise_subset = noisy_results_df[noisy_results_df['Noise_Type'] == noise_type]\n",
    "        noise_levels_sorted = sorted(noise_subset['Noise_Level'].unique())\n",
    "\n",
    "        fp32_accs = [noise_subset[noise_subset['Noise_Level'] == nl]['FP32_Accuracy'].iloc[0]\n",
    "                    for nl in noise_levels_sorted]\n",
    "        ptq_accs = [noise_subset[noise_subset['Noise_Level'] == nl]['PTQ_Accuracy'].iloc[0]\n",
    "                   for nl in noise_levels_sorted]\n",
    "        qat_accs = [noise_subset[noise_subset['Noise_Level'] == nl]['QAT_Accuracy'].iloc[0]\n",
    "                   for nl in noise_levels_sorted]\n",
    "\n",
    "        ax = axes[2, i]\n",
    "\n",
    "        ax.plot(noise_levels_sorted, fp32_accs, marker='o', linewidth=3, markersize=8,\n",
    "                color='#1f77b4', label='FP32')\n",
    "        ax.plot(noise_levels_sorted, ptq_accs, marker='s', linewidth=3, markersize=8,\n",
    "                color='#ff7f0e', label='PTQ')\n",
    "        ax.plot(noise_levels_sorted, qat_accs, marker='^', linewidth=3, markersize=8,\n",
    "                color='#2ca02c', label='QAT')\n",
    "\n",
    "        # Add clean baselines as horizontal lines\n",
    "        ax.axhline(y=fp32_accuracy_clean, color='#1f77b4', linestyle='--', alpha=0.5,\n",
    "                  label='FP32 Clean')\n",
    "        ax.axhline(y=ptq_accuracy_clean, color='#ff7f0e', linestyle='--', alpha=0.5,\n",
    "                  label='PTQ Clean')\n",
    "        ax.axhline(y=qat_accuracy_clean, color='#2ca02c', linestyle='--', alpha=0.5,\n",
    "                  label='QAT Clean')\n",
    "\n",
    "        ax.set_title(f'Performance vs {noise_type.title()} Noise Level', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Noise Level')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Set reasonable y-axis limits\n",
    "        all_accs = fp32_accs + ptq_accs + qat_accs\n",
    "        ax.set_ylim([min(all_accs) - 2, max(all_accs) + 2])\n",
    "\n",
    "    # Fill empty subplot if only one noise type\n",
    "    if len(plot_noise_types) < 2:\n",
    "        axes[2, 1].text(0.5, 0.5, 'Additional noise type\\nnot available',\n",
    "                       ha='center', va='center', transform=axes[2, 1].transAxes)\n",
    "        axes[2, 1].set_title('Performance vs Noise Level (No Data)', fontsize=14)\n",
    "\n",
    "else:\n",
    "    for i in range(2):\n",
    "        axes[2, i].text(0.5, 0.5, 'No noise data available\\nfor trend analysis',\n",
    "                       ha='center', va='center', transform=axes[2, i].transAxes)\n",
    "        axes[2, i].set_title(f'Performance vs Noise Level (No Data)', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a58a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY STATISTICS AND KEY INSIGHTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"1. COMPRESSION ANALYSIS:\")\n",
    "print(f\"   FP32 Size: {fp32_size:.2f} MB\")\n",
    "print(f\"   PTQ Size: {ptq_size:.2f} MB ({fp32_size/ptq_size:.1f}x smaller)\")\n",
    "print(f\"   QAT Size: {qat_size:.2f} MB ({fp32_size/qat_size:.1f}x smaller)\")\n",
    "\n",
    "print(f\"\\n2. SPEED ANALYSIS:\")\n",
    "print(f\"   FP32 Latency: {fp32_latency:.2f} ms\")\n",
    "print(f\"   PTQ Latency: {ptq_latency:.2f} ms ({fp32_latency/ptq_latency:.1f}x faster)\")\n",
    "print(f\"   QAT Latency: {qat_latency:.2f} ms ({fp32_latency/qat_latency:.1f}x faster)\")\n",
    "\n",
    "print(f\"\\n3. CLEAN DATA ACCURACY:\")\n",
    "print(f\"   FP32: {fp32_accuracy_clean:.2f}%\")\n",
    "print(f\"   PTQ: {ptq_accuracy_clean:.2f}% (drop: {fp32_accuracy_clean-ptq_accuracy_clean:.2f}%)\")\n",
    "print(f\"   QAT: {qat_accuracy_clean:.2f}% (drop: {fp32_accuracy_clean-qat_accuracy_clean:.2f}%)\")\n",
    "\n",
    "if len(noisy_results_df) > 0:\n",
    "    print(f\"\\n4. NOISE ROBUSTNESS ANALYSIS:\")\n",
    "\n",
    "    # Calculate average degradation across all noise conditions\n",
    "    fp32_avg_noisy = noisy_results_df['FP32_Accuracy'].mean()\n",
    "    ptq_avg_noisy = noisy_results_df['PTQ_Accuracy'].mean()\n",
    "    qat_avg_noisy = noisy_results_df['QAT_Accuracy'].mean()\n",
    "\n",
    "    print(f\"   Average accuracy across all noise conditions:\")\n",
    "    print(f\"   FP32: {fp32_avg_noisy:.2f}% (degradation: {fp32_accuracy_clean-fp32_avg_noisy:.2f}%)\")\n",
    "    print(f\"   PTQ: {ptq_avg_noisy:.2f}% (degradation: {ptq_accuracy_clean-ptq_avg_noisy:.2f}%)\")\n",
    "    print(f\"   QAT: {qat_avg_noisy:.2f}% (degradation: {qat_accuracy_clean-qat_avg_noisy:.2f}%)\")\n",
    "\n",
    "    # Find best and worst performing conditions\n",
    "    worst_condition = noisy_results_df.loc[noisy_results_df['FP32_Accuracy'].idxmin()]\n",
    "    best_condition = noisy_results_df.loc[noisy_results_df['FP32_Accuracy'].idxmax()]\n",
    "\n",
    "    print(f\"\\n   Worst performing condition: {worst_condition['Noise_Type']} @ {worst_condition['Noise_Level']}\")\n",
    "    print(f\"   FP32: {worst_condition['FP32_Accuracy']:.1f}%, PTQ: {worst_condition['PTQ_Accuracy']:.1f}%, QAT: {worst_condition['QAT_Accuracy']:.1f}%\")\n",
    "\n",
    "    print(f\"\\n   Best performing condition: {best_condition['Noise_Type']} @ {best_condition['Noise_Level']}\")\n",
    "    print(f\"   FP32: {best_condition['FP32_Accuracy']:.1f}%, PTQ: {best_condition['PTQ_Accuracy']:.1f}%, QAT: {best_condition['QAT_Accuracy']:.1f}%\")\n",
    "\n",
    "print(f\"\\n5. KEY TAKEAWAYS:\")\n",
    "print(f\"   ✓ Quantization achieves ~{fp32_size/ptq_size:.0f}x compression and ~{fp32_latency/ptq_latency:.0f}x speedup\")\n",
    "print(f\"   ✓ QAT recovers {abs(ptq_accuracy_clean - qat_accuracy_clean):.1f}% accuracy vs PTQ on clean data\")\n",
    "if len(noisy_results_df) > 0:\n",
    "    qat_better_count = sum(noisy_results_df['QAT_Accuracy'] > noisy_results_df['PTQ_Accuracy'])\n",
    "    qat_better_pct = (qat_better_count / len(noisy_results_df)) * 100\n",
    "    print(f\"   ✓ QAT outperforms PTQ in {qat_better_pct:.0f}% of noisy conditions\")\n",
    "    print(f\"   ⚠ All models show degradation under noise, quantized models may be more sensitive\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
